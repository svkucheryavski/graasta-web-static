const appBlocks = [{"title": "Descriptive statistics and plots", "apps": [{"id": "asta-b101", "title": "Quantiles, quartiles, percentiles", "info": "How to compute simple statistics for a sample.", "video": "https://www.youtube.com/embed/l_tnFhwRpjI", "help": "<h2>Quantiles, quartiles, percentiles</h2><p>This app shows calculation of main non-parametric descriptive statistics: <i>min</i>, <i>max</i>, <i>quartiles</i> and <i>percentils</i>. The plot contains current sample values as points and the traditional box and whiskers plot. The dashed line inside the box shows the mean. The red elements represent boundaries for detection of outliers (based on \u00b11.5IQR rule).</p><p>Try to change the smallest (<i>min</i>) or the largest (<i>max</i>) values of your current sample using the sliders in order to see what happens to the boxplot if one of the values will be outside the boundaries. You can also pay attention which statistics are changing and which remain stable in this case.</p><p>The table in the bottom shows the current values (<i>x</i>) ordered from smallest to largest, their rank (<i>i</i>), as well as their percentiles (<i>p</i>) also known as <em>sample quantiles</em>. The percentiles are computed using <code>(i - 0.5)/n</code> rule. The table on the right side shows the computed statistics.</p>"}, {"id": "asta-b102", "title": "Samples and populations", "info": "How a sample taken from a population looks like.", "video": "https://www.youtube.com/embed/hhGmFVMm5ZE", "help": "<h2>Samples and populations</h2> <p>This app helps you to investigate how different a sample can be when it is being randomly taken from corresponding population.</p> <p>You can investigate this difference for one of the three parameters: Height, Age and IQ of a population of people. Each parameter has own distribution. Thus, <em>Age</em> is distributed uniformly, <em>IQ</em> is distributed normally and <em>Height</em> has distribution with two peaks (bimodal). You can also see how sample size influences the difference.</p>  <p>Plot series made for a population (histogram and boxplot on the left part and percentile plot on the right) are shown using gray colors. The size of the population is <em>N</em> = 50&nbsp;000. The plot series for current sample are shown in blue. A new sample is taken when you change any of the controls \u2014 select the population parameter or the sample size as well as when you force to take a new sample by clicking the specific button.</p>"}, {"id": "asta-b103", "title": "PDF, CDF and ICDF", "info": "Main functions for theoretical distributions.", "video": "https://www.youtube.com/embed/lS2iK4Rymy4 ", "help": "<h2>PDF, CDF, and ICDF</h2>  <p>This app lets you play with three main functions available for any theoretical distribution: <em>Probability Density Function</em> (PDF), <em>Cumulative Distribution Function</em> (CDF) and <em>Inverse Cumulative Distribution Function</em> (ICDF). The functions can be used for different purposes. Thus PDF shows a shape of distribution in form of a density of the values, the higher density \u2014 the bigger chance that your random value will be there. For example, in case of normal distribution, the highest density is around <em>mean</em>, so mean is the most expected value in this case.</p> <p>CDF function gives you a chance to get a value smaller than given. While the ICDF does the opposite \u2014 gives you a value for a given probability. The functions in this app can be used in \"Value\" mode, for a single value, as well as in \"Interval\" mode for an interval limited by two values.</p> <p>For example, we are talking about height of people, normally distributed with mean = 170 cm and std = 10 cm (initial settings of the app). What is a chance that a random person from this population will have height between 160 and 180 cm? Or, in other words, how many people in percent have height between these two values in the population? Just set <em>x</em><sub>1</sub> to 160 and <em>x</em><sub>2</sub> to 180 under the CDF plot and you will see the result (in this case the chance is around 0.683 or 68.3%).</p>"}, {"id": "asta-b104", "title": "Quantile-quantile plot", "info": "How to create and interpret a QQ-plot.", "video": "https://www.youtube.com/embed/G12DrRZAPHA", "help": "<h2>Quantile-quantile plot</h2><p>This app shows how to use quantile-quantile (QQ) plot to check if your values came from normally distributed population. In this case the values (height of people, <em>x</em>) are indeed randomly taken from a population, where they follow normal distribution with mean = 170 cm and standard deviation = 10 cm. The values of the current sample are shown in the large table as row <em>x</em> and on the plot as y-axis values.</p> <p>First, for every value <em>x</em> we compute probability <em>p</em>, to get a value even smaller, similar to what we did when computed percentiles. In this case we use <code>p = (i - 0.5) / n</code>. But if sample size is smaller than 10, the formula is slightly different: <code>p = (i - 0.375) / (n + 0.25)</code>. For example, if sample size = 6, then the first value (i = 1) will have the following p: <code>p = (1 - 0.375) / (6 + 0.25) = 0.100</code>.</p> <p>After that, for every <em>p</em> we find corresponding standard score, <em>z</em>, using ICDF function for normal distribution. For example, if p = 0.100, the z-score can be found to be equal to -1.28. You can check it using app for PDF/CDF/ICDF or in R by running <code>qnorm(0.100)</code>. Finally we make a plot where sample values, <em>x</em> are shown as y-axis and the <em>z</em>-scores are shown as x-axis. In case if values follow normal distribution ideally they have linear dependence on z-scores, so the points will lie close to a straight line, shown as blue. The closer real points are to this line the more likely that they came from normally distributed population.</p>"}]}, {"title": "Confidence intervals", "apps": [{"id": "asta-b201", "title": "Population based CI for proportion", "info": "Confidence interval for proportion, based on population parameter.", "video": "", "help": "<h2>Population based confidence interval for proportion</h2> <p>This app allows you to play with proportion of a random sample. Here we have a population with N = 1600 individuals. Some of them are red, some are blue. You can change the proportion of the red individuals as you want (by default it is 50%). The population is shown as large plot on the left.</p> <p>If we know proportion of population and sample size we can compute an interval of expected proportions of the future samples. So, when you take a new random sample of that size from the population, its proportion will likely to be inside the interval. This interval is called <em>confidence interval for proportion</em> and since we compute it based on proportion parameter, it is <em>population based</em>.</p> <p>The interval for selected population proportion and current sample size computed for 95% confidence level is shown as a red area under a distribution curve on the right. The vertical line on that plot is a proportion of your current sample. Try to take many samples and see how often the proportion of the sample will be inside the interval (text on the plot shows this information). If you repeat this many (hundreds) times, about 95% of the samples should have proportion within the interval. <strong>However this works only if number of individuals in each group is at least 5.</strong> So if proportion is 10% you need to have sample size n = 50 to meat this requirement.</p>"}, {"id": "asta-b202", "title": "Sample based CI for proportion", "info": "Confidence interval for proportion, based on sample statistic.", "video": "https://www.youtube.com/embed/3lQRSkjL5ac", "help": "<h2>Sample based confidence interval for proportion</h2> <p>This app is similar to <code>asta-b201</code>, but, in this case, confidence interval is computed based on sample proportion. This requires larger sample size, so for every category you need at least 10 individuals in your sample. For example, if proportion is 20%, you need sample size of at least n = 50 to make a reliable interval (20% of 50 is 10). For p = 10% the sample size should be n = 100.</p> <p>The app shows 95% confidence interval computed for current sample as a plot on the right side. So, every time you take a new sample, this also results in a new confidence interval. The vertical red line on this plot shows the population proportion, which in real life we do not know. If you take a new sample many times (say, 200-300) you can see how often the population proportion, \u03c0, was inside the interval. If sample size is large enough it should be close to 95% \u2014\u00a0the confidence level.</p>"}, {"id": "asta-b203", "title": "Population based CI for mean", "info": "Confidence interval for mean, based on population parameter.", "video": "https://www.youtube.com/embed/cX8ErwtKMc8", "help": "<h2>Population based confidence interval for mean</h2> <p>This app is similar to <code>asta-b201</code> but is made to give you an idea about uncertainty of sample mean. Here we have a normally distributed population \u2014\u00a0concentration of Chloride in different parts of a water source. The concentration has a fixed mean, <em>\u00b5</em> = 100 mg/L, and a standard deviation, <em>\u03c3</em>, which you can vary from 1 to 5 mg/L. The population distribution is shown using gray colors on the left plot. Blue points on that plot show values of a current sample, randomly taken from the population. The vertical lines show the corresponding means.</p> <p>If we know mean of population, <em>\u00b5</em>, and sample size, we can compute an interval of expected mean values of the future samples, <em>m</em>. So, when you take a new random sample of that size from the population, its mean value will likely to be inside the interval. This interval is called <em>confidence interval for mean</em>and since we compute it based on population parameter, it is <em>population based</em>.</p> <p>Right plot shows distribution of possible mean values of samples to be randomly taken from the current population (and for current sample size). Confidence interval, computed for 95% confidence level is shown as a gray area under the distribution curve. The blue vertical line on that plot is a mean of your current sample. Try to take many samples and see how often the mean of a sample will be inside the interval (table under the plot shows this information). If you repeat this many (hundreds) times, about 95% of the samples should have mean within the interval.</p>"}, {"id": "asta-b204", "title": "Sample based CI for mean", "info": "Confidence interval for mean, based on sample statistics.", "video": "https://www.youtube.com/embed/EgE6-NNyyPc", "help": "<h2>Sample based confidence interval for mean</h2> <p>This app is similar to <code>asta-b203</code>, but in this case confidence interval for mean is computed using sample statistics, so we pretend we do not know the population mean and want to estimate it as a value located inside this interval. Thus on the right plot you see distribution and 95% confidence interval computed for current sample. The population mean (which in real life is unknown) is shown as a vertical line.</p> <p>        Try to take many samples and see how often mean of the population will be inside confidence interval computed for the sample. If you repeat this many (hundreds) times, about 95% of the samples will have interval, which contains the population mean. So, before you take a new sample you have 95% chance that confidence interval, computed around the sample mean, will contain the population mean.</p> <p>In this case we use Student's t-distribution to compute the interval. For given confidence level (e.g. 95%) and for given sample size (e.g. 5) we define a critical t-value \u2014\u00a0how many standard errors the interval will span on each side of the sample mean. E.g. for n = 5 this value is 2.78. You can see this value for current sample size in the table with statistics. If you have R you can also compute this value using ICDF function for t-distribution: <code>qt(0.975, 4)</code>. Here 0.975 is the right boundary of 95% interval and 4 is a number of degrees of freedom, which in this case is equal to <nobr>n - 1</nobr>.</p>"}]}, {"title": "Hypothesis testing", "apps": [{"id": "asta-b205", "title": "What is p-value?", "info": "Explanation of p-value using coin experiment.", "video": "https://www.youtube.com/embed/6O7rExp8tCQ", "help": "<h2>What is p-value?</h2><p>This app helps to understand the meaning of a p-value in hypotheses testing:</p> <p><em>p-value is a chance to get a sample as extreme as the one you have or even more extreme assuming that the null hypothesis (H0) is true.</em></p> <p>In case if all outcomes of an experiment are equally likely, to compute a p-value we need to know: <em>N1</em> \u2014 number of possible outcomes which will be as extreme as the one we currently have, <em>N2</em> \u2014 number of outcomes which will be more extreme for given H0, and <em>N</em> \u2014 total number of all possible outcomes. In this case the p-value can be computed as: <strong>p = (N1 + N2)/N</strong>.</p> <p>However, when we deal with continuous variables, number of possible outcomes is infinite and different outcomes may have different probabilities, therefore we have to use theoretical distributions for computing chances, which will be also shown in next apps. But in this app we introduce p-values based on experiment with limited number of outcomes \u2014\u00a0tossing a balanced coin several times (4 or 6). So we can count <em>N1</em>, <em>N2</em> and <em>N</em> and compute the p-value manually.</p>"}, {"id": "asta-b206", "title": "Test for sample proportion", "info": "How test for proportion works.", "video": "https://www.youtube.com/embed/zU3K4WWx7dI", "help": "<h2>Test for sample proportion</h2><p>This app visualizes a test for proportion of a sample \u2014 how likely the current sample came from population with given H0. In this case H0 is true, our population indeed has a proportion, \u03c0, which we set manually in the app (the population is shown on the left plot). So we expect that the test will confirm the H0 most of the time.</p> <p>Every time you take a new sample, app computes standard error and makes sampling distribution of possible        proportions around \u03c0 using the computed standard error and normal distribution. After that it evaluates how extreme your sample is and results in a p-value \u2014\u00a0chance to get a sample with proportion like you have or even more extreme assuming that H0 is true. If you take many samples, e.g. 200 or 300, then only 5% will have a p-value below 0.05, you can see all statistics right on the plot.</p> <p>However, this will work only if sample size is large enough. Try to set the population proportion to \u03c0 = 0.05 or 0.95. You will see that in this case even sample with n = 40 is too small for the test \u2014 sampling distribution curve will be truncated on one side. This leads to two problems \u2014\u00a0you will see an extreme p-value more often than expected and you have a chance to get a sample with members only from one group, so the sample proportion will be either 0 or 1. In this case standard error is 0 and there is no possibility to make a test. You need much larger sample to make a reliable test for such cases.</p>"}, {"id": "asta-b207", "title": "One sample t-test", "info": "Test for mean of one sample.", "video": "https://www.youtube.com/embed/PuIns8Y3gjI", "help": "<h2>One-sample t-test</h2><p>This app helps to understand how does the one sample t-test work. Here we have a normally distributed population \u2014 concentration of Chloride in different parts of a water source. The null hypothesis in this case is made about the population mean, \u00b5, and, depending on a tail, you have the following options \u2014 \"both\": H0: \u00b5 = 100 mg/L, \"left\": \u00b5 \u2265 100 mg/L, and \"right\": \u00b5 \u2264 100 mg/L. The population in this app has \u00b5 exactly equal to 100 mg/L, so all three hypothesis are true in this case. You have a possibility to change the standard deviation of the population, which by default is set to 3 mg/L but you will see, that it does not influence the outcome of the test.</p> <p>Then you can take a random sample from this population and see how far the mean of the sample is from the mean of the population. The app computes a chance to get a sample as extreme as given or even more extreme assuming that H0 is correct \u2014\u00a0the <strong>p-value</strong>. Usually p-value is used to assess how extreme your particular sample is for being taken from population where H0 is true. If p-value is small, it is considered as unlikely event and H0 is rejected.</p> <p>Often researchers use 5% (0.05) as a threshold for that. It is called <em>significance limit</em>. You will see that if you take many samples (100 or more), you will find out that approximately 5% of the samples will have p-value below 0.05 although the H0 is true. And this happens regardless the sample size. So this threshold is simply a chance to make a wrong decision by rejection the correct H0. So, if you use 0.05 you have 5% chance to make a wrong decision and e.g. \"see\" an effect, which does not exist.</p>"}, {"id": "asta-b208", "title": "Power of test and Type II error", "info": "How often you will be able to reject wrong H0.", "video": "https://www.youtube.com/embed/zUS5HDe5lMk", "help": "<h2>Power of test and Type II errors</h2> <p>This app is similar to <code>asta-b207</code> where you played with one-sample t-test. However, in this case you can emulate situations when H0 is not true, meaning the true population mean, \u00b5 is different from what you expect by setting H0. The possibilities for H0 are the same, depending on a tail, you have the following options \u2014 \"left\": \u00b5 \u2265 100 mg/L, and \"right\": \u00b5 \u2264 100 mg/L. But now you can also change the real population mean and set it to be smaller or larger than 100 mg/L.</p> <p>Try to do this and check how often you will be able to reject H0 (in this case we work with significance level 0.05, so we reject H0 when p-value is below this value). A probability to reject wrong H0 is called a <strong>power of test</strong>. And the situation when you can not reject it is called <strong>Type II</strong> error or false negative. The probability to get Type II error is always opposite to the power of test, e.g. if power is 80% you have 20% chance to make a Type II error.</p> <p>The power of any test depends on several things. First of all it is the test itself \u2014\u00a0different methods have different power. Second, it depends on the <strong>size of effect</strong> \u2014 difference between H0 mean and the real population mean (H1). E.g. if H0 assumes that \u00b5 \u2264 100 and the real \u00b5 = 105, this difference is 5. Finally, power also depends on standard deviation of your population as well as on the sample size. The last has very important consequence \u2014\u00a0the smaller effect you want to detect, the larger sample size should be.</p>"}]}, {"title": "Comparing means", "apps": [{"id": "asta-b209", "title": "Two sample t-test", "info": "How to compare mean of two samples.", "video": "https://www.youtube.com/embed/OEA5l04eVdU", "help": "<h2>Two sample t-test</h2><p>This app shows how to compare means of two samples. In this case the objective is to find out if the samples were taken from populations with the same means (H0: \u00b51 = \u00b52) or not (H1: \u00b51 \u2260 \u00b52). Here we use this test to see if increasing a temperature influence the yield of a chemical reaction. So, the population 1 consists of all possible outcomes of the reaction running at T = 120\u00baC. The population 2 consists of all possible outcomes of the reaction running at T = 160\u00baC. We assume that there are no other systematic factors involved so the variation of yield within each population is totally random and is distributed normally. The left plot shows the corresponding distributions using blue and red colors.</p> <p>By default \u00b51 = \u00b52 = 100 mg. Since \u00b51 \u2013 \u00b52 = 0, we can say that in this case <em>temperature does not have any effect on yield</em>. However, if we run the reactions just a few times (e.g. 3 for each temperature) you will always observe an effect and therefore you need to asses how likely you observe it just by chance.</p> <p>Use the app and investigate how often you will see an effect, which is not present and, vice versa, how often you will not be able to detect an existent effect. Check how the real (expected) effect size, noise and sample size influence this ability. The app works using significance level 0.05 but remember that for real applications it is better to use smaller value for the level.</p>"}, {"id": "asta-b210", "title": "Multiple comparison and Bonferroni correction", "info": "What if we apply t-test to more than 2 groups.", "video": "https://www.youtube.com/embed/1qh7Ibfeveg", "help": "<h2>Multiple t-test and Bonferroni correction</h2> <p>This app shows how to compare three samples taken from three populations. The three populations are all outcomes (yield measured in mg/L) of a chemical process running with a catalyst A, catalyst B and catalyst C. Here H0: \u00b5A = \u00b5B = \u00b5C = 100 mg/L. This means that regardless which catalyst we use, the average yield of the reaction is 100 mg/L, so changing catalyst has no effect on the yield. But when we run the reaction only 5 times for each catalyst, like shown in the app, the mean of these 5 runs will not be the same as the expected mean of the populations. And most of the time you will observe a difference among the sample means. Our goal is to use a t-test to test the H0 and make decision.</p> <p>However, t-test can be applied for comparing mean of two samples, while here we have three. One of the possibility will be to run t-test three times \u2014\u00a0one for each pair. This is what is called a <em>multiple compare</em> \u2014\u00a0you compare samples using several tests to check a single hypothesis. But the more tests you do the higher chance that you will reject correct H0. Try to run the test many times and you will see that although app works at significance limit 0.05 (so we expect that the H0 will be incorrectly rejected in 5% of cases), the real percent of rejections will be higher, about 10%.</p><p>You can overcome this problem by using Bonferroni correction, which decreases the significance limit in each individual tests, so the overall significance will be 0.05 (or any other pre-defined value). You can see the effect of correction by turning it on in the app and repeating the sampling many times again. In this case the significance level for individual tests will be set to 0.05/3 \u2248\u00a00.017 and the number of incorrectly rejected H0 will be around 5%.</p>"}, {"id": "asta-b211", "title": "One-way ANOVA (simplified)", "info": "How Analysis of Variance works for one factor.", "video": "https://www.youtube.com/embed/NMaIEHWkI5A", "help": "<h2>One way ANOVA</h2> <p>This app shows how one-way ANOVA tests means of three samples \u2014 the outcomes of a chemical reaction running using three different catalysis: <em>A</em>, <em>B</em> and <em>C</em>. We \"run\" the reaction with each catalyst 5 times, which gives 15 values \u2014 yield of each run in mg. The obtained yield values are shown in the top left table. The last row shows the average yield for each catalyst. You can adjust the expected effect for each catalyst and noise using slider controls.</p> <p>Then app computes a global mean for all original values and subtract it from the values thus creating a table with unbiased values, which are shown in the gray column. Table in the top of the column contains the unbiased values and their means. Under the table there are statistics: degrees of freedom (DoF), sum of squared values (SSQ) and variance or mean squares (MS = SSQ/DoF). Plot below shows boxplots for populations and points for the values.</p> <p>After that we split the unbiased values into a sum of <em>systematic</em> part, shown in the green column, and the <em>residuals</em>,\u00a0shown in the red column. In the systematic part we assume there is no noise, so all outcomes for given factor level (e.g. column A) have the same value \u2014 the corresponding mean. Residuals are computed as a difference between the unbiased values and the systematic part. App computes DoF, SSQ and MS for each part and the F-value \u2014\u00a0which is a ratio of MS for systematic part and residuals. The F-value follows F-distribution shown under the original data table. We use this distribution to compute corresponding p-value and make decision about the H0.</p>"}, {"id": "asta-b212", "title": "One-way ANOVA (full)", "info": "A more detailed app.", "video": "https://www.youtube.com/embed/k738X17uNUc", "help": "<h2>One way ANOVA (full)</h2> <p>This app is almost identical to the <code>asta-b211</code> but here we show calculations as they are without subtracting the global mean in advance. The results are absolutely identical but this time without additional step of unbiasing the values. Plus the app shows importance of QQ plot for residuals which helps to assess their normality.</p>"}]}, {"title": "Relationship between two variables", "apps": [{"id": "asta-b301", "title": "Covariance", "info": "How to compute and understand the covariance.", "video": "", "help": "<h2>Covariance</h2> <p>This app helps to understand covariance \u2014\u00a0a statistic which tells if two variables, <em>x</em> and <em>y</em> have a linear relationship (co-vary). If covariance is positive, then increasing <em>x</em> will likely lead to increasing of <em>y</em> value and vice versa. To compute the covariance, we first calculate distance from x- and y-value of a data point to corresponding means and then take a product of the two distances. The covariance is a sum of the distance products divided to the number of degrees of freedom (n - 1). You can see all these calculations in a table.</p> <p> Try to change parameters of a population: amount of noise and a slope of best fit line which has mean values as the origin. You will see how this influences your sample, and the sample co-variance. If product of two distances is positive this point contributes positively to the covariance and such point and the corresponding row in the table is shown using red color. If product of the two distances is negative \u2014\u00a0blue color is used.</p>"}, {"id": "asta-b302", "title": "Correlation and population based CI", "info": "Pearson's correlation coefficient and population based CI.", "video": "", "help": "<h2>Correlation and population based confidence interval</h2> <p>This app helps you to understand the Pearson's correlation coefficient, <em>r(x,y)</em>, which is computed as covariance for standardized <em>x</em> and <em>y</em> values. Alternatively you can compute covariance for the original values and then standardize the covariance by dividing it to the standard deviation of  <em>x</em> and <em>y</em>. If there is no noise at all, and <em>y</em> is linearly dependent on <em>x</em>, the correlation does not depend on slope of the line. However, when noise is present, the slope has an influence which you can see by playing with the app. The right column in the table with statistics (shown as gray) shows values for population, the middle column shows values for a current sample.</p> <p>The uncertainty for correlation coefficient of a sample depends both on the correlation of population and the sample size. The sample correlation coefficient does not follow any theoretical distribution, therefore for computing the uncertainty and corresponding confidence interval, a <a href=\"https://en.wikipedia.org/wiki/Fisher_transformation\">transformed statistic</a>, <em>z'</em>, is used. This statistic follows normal distribution if n > 10. The app shows how the distribution of <em>z'</em> looks like for different levels of noise and how it can be transformed back to distribution of <em>r</em> values.</p>"}, {"id": "asta-b303", "title": "Correlation and sample based CI", "info": "Pearson's correlation coefficient and sample based CI.", "video": "", "help": "<h2>Correlation and sample based confidence interval</h2> <p>This app is almost identical to the previous one (asta-b302) with one important difference: confidence interval in this app is computed based on statistics of a current sample. So, you can see how confidence interval vary from one sample to another and how often the correlation coefficient of population (or it's trasformed value, z') will be inside the interval.</p> <p>Because the confidence intervals in this app are computed for 95% confidence level, you can expect that in 95% of all cases sample will contain the population parameter inside the interval. However, you will see exactly 95%, only if you take a large amount of samples, several hundreds or even thousands. This is similar to confidence intervals computed for other statistics, e.g. mean or proportion.</p>"}, {"id": "asta-b304", "title": "Simple linear regression", "info": "SLR and its main outcomes.", "video": "", "help": "<h2>Simple linear regression</h2> <p>The app shows how to use simple linear regression for investigation of relationship between two variables (in this case height and weight of adult persons). The plot shows data points both for population (N = 500) and current sample (n = 10). Both sets of points are fitted by a simple linear regression model, you can see both models in form of lines and the corresponding equations, as well as their characteristics (standard error of  prediction and coefficient of determination, R2). The table on the right part of the app shows reference y-values, values, predicted by the model, error of prediction and its square. Sum of squared errors is what is used to compute both standard error and R2.</p> <p>The shaded area on the plot shows uncertainties. By default you see uncertainty from both fitting and sampling error. You can use the switch to see uncertainty from one of the source. You can also change the amount of noise (the more noise, the less percent of y-variance can be predicted by the model) and see how it changes the uncertainties. Plus you can select any sample point on the plot and see the predicted value and the uncertainty interval for this point.</p>"}, {"id": "asta-b305", "title": "Sampling error and overfitting", "info": "How sampling error depends on sample size and model complexity.", "video": "", "help": "<h2>Sampling error and overfitting</h2> <p>This app shows how sample size and complexity of a regression model influence the sampling error. Sampling error in this case can be defined as variation of regression coefficients of a model, trained on a sample, around the \"true\" regression coefficients of a model trained on the population points. This app uses polynomial model for regression \u2014 the higher polynomial degree the higher the model complexity.</p> <p>Just set the desired sample size and the polynomial degree and then start collecting new samples. Points and model for the current sample are shown using red color for better contrast. The models for all previous samples are kept on the main plot (they are also red but semi transparent), so you can see how big the variation of the models is.</p> <p>The small plot on the right shows regression coefficients. The semi-transparent blue bars show the \"true\" regression coefficients for the population. Red points are regression coefficients of current and all previous samples. So you can see how big the variation of the coefficients is and how it depends on sample size and model complexity.</p>"}]}]; export default appBlocks;