var app=function(){"use strict";function e(){}const t=e=>e;function o(e,t){for(const o in t)e[o]=t[o];return e}function n(e){return e()}function a(){return Object.create(null)}function i(e){e.forEach(n)}function s(e){return"function"==typeof e}function r(e,t){return e!=e?t==t:e!==t||e&&"object"==typeof e||"function"==typeof e}let l;function h(e,t){return l||(l=document.createElement("a")),l.href=t,e===l.href}const c="undefined"!=typeof window;let p=c?()=>window.performance.now():()=>Date.now(),d=c?e=>requestAnimationFrame(e):e;const u=new Set;function m(e){u.forEach((t=>{t.c(e)||(u.delete(t),t.f())})),0!==u.size&&d(m)}function f(e,t){e.appendChild(t)}function w(e){if(!e)return document;const t=e.getRootNode?e.getRootNode():e.ownerDocument;return t&&t.host?t:e.ownerDocument}function b(e){const t=T("style");return function(e,t){f(e.head||e,t)}(w(e),t),t.sheet}function v(e,t,o){e.insertBefore(t,o||null)}function g(e){e.parentNode.removeChild(e)}function y(e,t){for(let o=0;o<e.length;o+=1)e[o]&&e[o].d(t)}function T(e){return document.createElement(e)}function k(e){return document.createTextNode(e)}function x(){return k(" ")}function $(e,t,o,n){return e.addEventListener(t,o,n),()=>e.removeEventListener(t,o,n)}function C(e,t,o){null==o?e.removeAttribute(t):e.getAttribute(t)!==o&&e.setAttribute(t,o)}function I(e,t){t=""+t,e.wholeText!==t&&(e.data=t)}function H(e,t){e.value=null==t?"":t}function A(e,t,o){e.classList[o?"add":"remove"](t)}function z(e,t,{bubbles:o=!1,cancelable:n=!1}={}){const a=document.createEvent("CustomEvent");return a.initCustomEvent(e,o,n,t),a}const P=new Map;let S,F=0;function L(e,t,o,n,a,i,s,r=0){const l=16.666/n;let h="{\n";for(let e=0;e<=1;e+=l){const n=t+(o-t)*i(e);h+=100*e+`%{${s(n,1-n)}}\n`}const c=h+`100% {${s(o,1-o)}}\n}`,p=`__svelte_${function(e){let t=5381,o=e.length;for(;o--;)t=(t<<5)-t^e.charCodeAt(o);return t>>>0}(c)}_${r}`,d=w(e),{stylesheet:u,rules:m}=P.get(d)||function(e,t){const o={stylesheet:b(t),rules:{}};return P.set(e,o),o}(d,e);m[p]||(m[p]=!0,u.insertRule(`@keyframes ${p} ${c}`,u.cssRules.length));const f=e.style.animation||"";return e.style.animation=`${f?`${f}, `:""}${p} ${n}ms linear ${a}ms 1 both`,F+=1,p}function _(e,t){const o=(e.style.animation||"").split(", "),n=o.filter(t?e=>e.indexOf(t)<0:e=>-1===e.indexOf("__svelte")),a=o.length-n.length;a&&(e.style.animation=n.join(", "),F-=a,F||d((()=>{F||(P.forEach((e=>{const{stylesheet:t}=e;let o=t.cssRules.length;for(;o--;)t.deleteRule(o);e.rules={}})),P.clear())})))}function q(e){S=e}function j(){const e=function(){if(!S)throw new Error("Function called outside component initialization");return S}();return(t,o,{cancelable:n=!1}={})=>{const a=e.$$.callbacks[t];if(a){const i=z(t,o,{cancelable:n});return a.slice().forEach((t=>{t.call(e,i)})),!i.defaultPrevented}return!0}}const E=[],D=[],N=[],R=[],Y=Promise.resolve();let M=!1;function B(e){N.push(e)}const O=new Set;let Q,V=0;function X(){const e=S;do{for(;V<E.length;){const e=E[V];V++,q(e),W(e.$$)}for(q(null),E.length=0,V=0;D.length;)D.pop()();for(let e=0;e<N.length;e+=1){const t=N[e];O.has(t)||(O.add(t),t())}N.length=0}while(E.length);for(;R.length;)R.pop()();M=!1,O.clear(),q(e)}function W(e){if(null!==e.fragment){e.update(),i(e.before_update);const t=e.dirty;e.dirty=[-1],e.fragment&&e.fragment.p(e.ctx,t),e.after_update.forEach(B)}}function U(e,t,o){e.dispatchEvent(z(`${t?"intro":"outro"}${o}`))}const J=new Set;let K;function G(){K={r:0,c:[],p:K}}function Z(){K.r||i(K.c),K=K.p}function ee(e,t){e&&e.i&&(J.delete(e),e.i(t))}function te(e,t,o,n){if(e&&e.o){if(J.has(e))return;J.add(e),K.c.push((()=>{J.delete(e),n&&(o&&e.d(1),n())})),e.o(t)}}const oe={duration:0};function ne(o,n,a,r){let l=n(o,a),h=r?0:1,c=null,f=null,w=null;function b(){w&&_(o,w)}function v(e,t){const o=e.b-h;return t*=Math.abs(o),{a:h,b:e.b,d:o,duration:t,start:e.start,end:e.start+t,group:e.group}}function g(n){const{delay:a=0,duration:s=300,easing:r=t,tick:g=e,css:y}=l||oe,T={start:p()+a,b:n};n||(T.group=K,K.r+=1),c||f?f=T:(y&&(b(),w=L(o,h,n,s,a,r,y)),n&&g(0,1),c=v(T,s),B((()=>U(o,n,"start"))),function(e){let t;0===u.size&&d(m),new Promise((o=>{u.add(t={c:e,f:o})}))}((e=>{if(f&&e>f.start&&(c=v(f,s),f=null,U(o,c.b,"start"),y&&(b(),w=L(o,h,c.b,c.duration,0,r,l.css))),c)if(e>=c.end)g(h=c.b,1-h),U(o,c.b,"end"),f||(c.b?b():--c.group.r||i(c.group.c)),c=null;else if(e>=c.start){const t=e-c.start;h=c.a+c.d*r(t/c.duration),g(h,1-h)}return!(!c&&!f)})))}return{run(e){s(l)?(Q||(Q=Promise.resolve(),Q.then((()=>{Q=null}))),Q).then((()=>{l=l(),g(e)})):g(e)},end(){b(),c=f=null}}}function ae(e,t){const o={},n={},a={$$scope:1};let i=e.length;for(;i--;){const s=e[i],r=t[i];if(r){for(const e in s)e in r||(n[e]=1);for(const e in r)a[e]||(o[e]=r[e],a[e]=1);e[i]=r}else for(const e in s)a[e]=1}for(const e in n)e in o||(o[e]=void 0);return o}function ie(e){return"object"==typeof e&&null!==e?e:{}}function se(e){e&&e.c()}function re(e,t,o,a){const{fragment:r,on_mount:l,on_destroy:h,after_update:c}=e.$$;r&&r.m(t,o),a||B((()=>{const t=l.map(n).filter(s);h?h.push(...t):i(t),e.$$.on_mount=[]})),c.forEach(B)}function le(e,t){const o=e.$$;null!==o.fragment&&(i(o.on_destroy),o.fragment&&o.fragment.d(t),o.on_destroy=o.fragment=null,o.ctx=[])}function he(e,t){-1===e.$$.dirty[0]&&(E.push(e),M||(M=!0,Y.then(X)),e.$$.dirty.fill(0)),e.$$.dirty[t/31|0]|=1<<t%31}function ce(t,o,n,s,r,l,h,c=[-1]){const p=S;q(t);const d=t.$$={fragment:null,ctx:null,props:l,update:e,not_equal:r,bound:a(),on_mount:[],on_destroy:[],on_disconnect:[],before_update:[],after_update:[],context:new Map(o.context||(p?p.$$.context:[])),callbacks:a(),dirty:c,skip_bound:!1,root:o.target||p.$$.root};h&&h(d.root);let u=!1;if(d.ctx=n?n(t,o.props||{},((e,o,...n)=>{const a=n.length?n[0]:o;return d.ctx&&r(d.ctx[e],d.ctx[e]=a)&&(!d.skip_bound&&d.bound[e]&&d.bound[e](a),u&&he(t,e)),o})):[],d.update(),u=!0,i(d.before_update),d.fragment=!!s&&s(d.ctx),o.target){if(o.hydrate){const e=function(e){return Array.from(e.childNodes)}(o.target);d.fragment&&d.fragment.l(e),e.forEach(g)}else d.fragment&&d.fragment.c();o.intro&&ee(t.$$.fragment),re(t,o.target,o.anchor,o.customElement),X()}q(p)}class pe{$destroy(){le(this,1),this.$destroy=e}$on(e,t){const o=this.$$.callbacks[e]||(this.$$.callbacks[e]=[]);return o.push(t),()=>{const e=o.indexOf(t);-1!==e&&o.splice(e,1)}}$set(e){var t;this.$$set&&(t=e,0!==Object.keys(t).length)&&(this.$$.skip_bound=!0,this.$$set(e),this.$$.skip_bound=!1)}}function de(e){let t,o,n;return{c(){t=T("a"),o=k("Watch"),C(t,"class","toolbar__link toolbar__link_watch svelte-1wsuemj"),C(t,"title","Video"),C(t,"href",n="#"+e[0]+"/video")},m(e,n){v(e,t,n),f(t,o)},p(e,o){1&o&&n!==(n="#"+e[0]+"/video")&&C(t,"href",n)},d(e){e&&g(t)}}}function ue(t){let o,n,a,i,s,r,l,h,c,p,d,u,m,w,b,y,$,H,A,z,P=t[3]&&de(t);return{c(){o=T("div"),n=T("span"),a=k(t[0]),i=x(),s=T("div"),r=T("h3"),l=k(t[1]),h=x(),c=T("p"),p=k(t[2]),d=x(),u=T("div"),m=T("a"),w=k("Try"),y=x(),P&&P.c(),$=x(),H=T("a"),A=k("Download"),C(n,"class","app-id svelte-1wsuemj"),C(r,"class","svelte-1wsuemj"),C(c,"class","svelte-1wsuemj"),C(m,"class","toolbar__link toolbar__link_try svelte-1wsuemj"),C(m,"title","Run demo"),C(m,"href",b="#"+t[0]+"/app"),C(H,"class","toolbar__link toolbar__link_download svelte-1wsuemj"),C(H,"title","Download"),C(H,"href",z="/apps/"+t[0]+".zip"),C(u,"class","toolbar svelte-1wsuemj"),C(s,"class","app-info svelte-1wsuemj"),C(o,"class","app-details svelte-1wsuemj")},m(e,t){v(e,o,t),f(o,n),f(n,a),f(o,i),f(o,s),f(s,r),f(r,l),f(s,h),f(s,c),f(c,p),f(s,d),f(s,u),f(u,m),f(m,w),f(u,y),P&&P.m(u,null),f(u,$),f(u,H),f(H,A)},p(e,[t]){1&t&&I(a,e[0]),2&t&&I(l,e[1]),4&t&&I(p,e[2]),1&t&&b!==(b="#"+e[0]+"/app")&&C(m,"href",b),e[3]?P?P.p(e,t):(P=de(e),P.c(),P.m(u,$)):P&&(P.d(1),P=null),1&t&&z!==(z="/apps/"+e[0]+".zip")&&C(H,"href",z)},i:e,o:e,d(e){e&&g(o),P&&P.d()}}}function me(e,t,o){let{id:n}=t,{title:a}=t,{info:i}=t,{help:s}=t,{video:r}=t;return e.$$set=e=>{"id"in e&&o(0,n=e.id),"title"in e&&o(1,a=e.title),"info"in e&&o(2,i=e.info),"help"in e&&o(4,s=e.help),"video"in e&&o(3,r=e.video)},[n,a,i,r,s]}class fe extends pe{constructor(e){super(),ce(this,e,me,ue,r,{id:0,title:1,info:2,help:4,video:3})}}function we(e,t,o){const n=e.slice();return n[2]=t[o],n}function be(t){let o;return{c(){o=T("p"),o.textContent="No apps available in this block.",C(o,"class","svelte-i7rmfd")},m(e,t){v(e,o,t)},p:e,d(e){e&&g(o)}}}function ve(e){let t,n,a;const i=[e[2]];let s={};for(let e=0;e<i.length;e+=1)s=o(s,i[e]);return n=new fe({props:s}),{c(){t=T("li"),se(n.$$.fragment),C(t,"class","svelte-i7rmfd")},m(e,o){v(e,t,o),re(n,t,null),a=!0},p(e,t){const o=2&t?ae(i,[ie(e[2])]):{};n.$set(o)},i(e){a||(ee(n.$$.fragment,e),a=!0)},o(e){te(n.$$.fragment,e),a=!1},d(e){e&&g(t),le(n)}}}function ge(e){let t,o,n,a,i,s,r=e[1],l=[];for(let t=0;t<r.length;t+=1)l[t]=ve(we(e,r,t));const h=e=>te(l[e],1,1,(()=>{l[e]=null}));let c=null;return r.length||(c=be()),{c(){t=T("article"),o=T("h2"),n=k(e[0]),a=x(),i=T("ul");for(let e=0;e<l.length;e+=1)l[e].c();c&&c.c(),C(o,"class","svelte-i7rmfd"),C(i,"class","svelte-i7rmfd"),C(t,"class","app-block svelte-i7rmfd")},m(e,r){v(e,t,r),f(t,o),f(o,n),f(t,a),f(t,i);for(let e=0;e<l.length;e+=1)l[e].m(i,null);c&&c.m(i,null),s=!0},p(e,[t]){if((!s||1&t)&&I(n,e[0]),2&t){let o;for(r=e[1],o=0;o<r.length;o+=1){const n=we(e,r,o);l[o]?(l[o].p(n,t),ee(l[o],1)):(l[o]=ve(n),l[o].c(),ee(l[o],1),l[o].m(i,null))}for(G(),o=r.length;o<l.length;o+=1)h(o);Z(),!r.length&&c?c.p(e,t):r.length?c&&(c.d(1),c=null):(c=be(),c.c(),c.m(i,null))}},i(e){if(!s){for(let e=0;e<r.length;e+=1)ee(l[e]);s=!0}},o(e){l=l.filter(Boolean);for(let e=0;e<l.length;e+=1)te(l[e]);s=!1},d(e){e&&g(t),y(l,e),c&&c.d()}}}function ye(e,t,o){let{title:n}=t,{apps:a}=t;return e.$$set=e=>{"title"in e&&o(0,n=e.title),"apps"in e&&o(1,a=e.apps)},[n,a]}class Te extends pe{constructor(e){super(),ce(this,e,ye,ge,r,{title:0,apps:1})}}function ke(e){const t=e-1;return t*t*t+1}function xe(e,{delay:o=0,duration:n=400,easing:a=t}={}){const i=+getComputedStyle(e).opacity;return{delay:o,duration:n,easing:a,css:e=>"opacity: "+e*i}}function $e(e,{delay:t=0,duration:o=400,easing:n=ke,x:a=0,y:i=0,opacity:s=0}={}){const r=getComputedStyle(e),l=+r.opacity,h="none"===r.transform?"":r.transform,c=l*(1-s);return{delay:t,duration:o,easing:n,css:(e,t)=>`\n\t\t\ttransform: ${h} translate(${(1-e)*a}px, ${(1-e)*i}px);\n\t\t\topacity: ${l-c*t}`}}function Ce(t){let o,n,a;return{c(){o=T("iframe"),n=k("Loading"),C(o,"title",t[0]),h(o.src,a="/apps/"+t[0]+"/index.html")||C(o,"src",a),C(o,"width","100%"),C(o,"height","100%"),C(o,"class","svelte-1tc7xux")},m(e,t){v(e,o,t),f(o,n)},p(e,[t]){1&t&&C(o,"title",e[0]),1&t&&!h(o.src,a="/apps/"+e[0]+"/index.html")&&C(o,"src",a)},i:e,o:e,d(e){e&&g(o)}}}function Ie(e,t,o){let{id:n}=t;return e.$$set=e=>{"id"in e&&o(0,n=e.id)},[n]}class He extends pe{constructor(e){super(),ce(this,e,Ie,Ce,r,{id:0})}}function Ae(e){let t,o,n;return{c(){t=T("div"),o=T("iframe"),C(o,"width","100%"),C(o,"height","100%"),h(o.src,n=e[2])||C(o,"src",n),C(o,"title","YouTube video player"),C(o,"frameborder","0"),C(o,"allow","accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"),o.allowFullscreen=!0,C(t,"class","content-container svelte-1uixep7"),A(t,"hidden","video"!=e[4])},m(e,n){v(e,t,n),f(t,o)},p(e,a){4&a&&!h(o.src,n=e[2])&&C(o,"src",n),16&a&&A(t,"hidden","video"!=e[4])},d(e){e&&g(t)}}}function ze(e){let t,o,n;return{c(){t=T("a"),o=k("Video"),C(t,"role","tab"),C(t,"href",n="#"+e[0]+"/video"),C(t,"class","svelte-1uixep7"),A(t,"selected","video"===e[4])},m(e,n){v(e,t,n),f(t,o)},p(e,o){1&o&&n!==(n="#"+e[0]+"/video")&&C(t,"href",n),16&o&&A(t,"selected","video"===e[4])},d(e){e&&g(t)}}}function Pe(e){let t,o,n;return{c(){t=T("a"),o=k("Info"),C(t,"role","tab"),C(t,"href",n="#"+e[0]+"/info"),C(t,"class","svelte-1uixep7"),A(t,"selected","info"===e[4])},m(e,n){v(e,t,n),f(t,o)},p(e,o){1&o&&n!==(n="#"+e[0]+"/info")&&C(t,"href",n),16&o&&A(t,"selected","info"===e[4])},d(e){e&&g(t)}}}function Se(e){let t,o,n,a,s,r,l,h,c,p,d,u,m,w,b,y,I,H,z,P,S,F,L,_,q;h=new He({props:{id:e[0]}});let j=e[2]&&Ae(e),E=e[2]&&ze(e),D=e[3]&&Pe(e);return{c(){t=T("div"),o=T("article"),n=T("header"),a=T("h2"),s=x(),r=T("section"),l=T("div"),se(h.$$.fragment),c=x(),j&&j.c(),p=x(),d=T("div"),u=T("div"),m=x(),w=T("footer"),b=T("nav"),y=T("a"),I=k("App"),z=x(),E&&E.c(),P=x(),D&&D.c(),C(a,"title","click to close"),C(a,"class","svelte-1uixep7"),C(n,"class","modal-header svelte-1uixep7"),C(l,"class","content-container svelte-1uixep7"),A(l,"hidden","app"!=e[4]),C(u,"class","helptext svelte-1uixep7"),C(d,"class","content-container svelte-1uixep7"),A(d,"hidden","info"!=e[4]),C(r,"class","modal-content svelte-1uixep7"),C(y,"role","tab"),C(y,"href",H="#"+e[0]+"/app"),C(y,"class","svelte-1uixep7"),A(y,"selected","app"===e[4]),C(b,"class","tablist svelte-1uixep7"),C(b,"role","tablist"),C(w,"class","modal-footer svelte-1uixep7"),C(o,"class","modal svelte-1uixep7"),C(t,"class","backstage svelte-1uixep7")},m(i,g){v(i,t,g),f(t,o),f(o,n),f(n,a),a.innerHTML=e[1],f(o,s),f(o,r),f(r,l),re(h,l,null),f(r,c),j&&j.m(r,null),f(r,p),f(r,d),f(d,u),u.innerHTML=e[3],f(o,m),f(o,w),f(w,b),f(b,y),f(y,I),f(b,z),E&&E.m(b,null),f(b,P),D&&D.m(b,null),L=!0,_||(q=[$(window,"keydown",e[6]),$(a,"click",e[8])],_=!0)},p(e,[t]){(!L||2&t)&&(a.innerHTML=e[1]);const o={};1&t&&(o.id=e[0]),h.$set(o),16&t&&A(l,"hidden","app"!=e[4]),e[2]?j?j.p(e,t):(j=Ae(e),j.c(),j.m(r,p)):j&&(j.d(1),j=null),(!L||8&t)&&(u.innerHTML=e[3]),16&t&&A(d,"hidden","info"!=e[4]),(!L||1&t&&H!==(H="#"+e[0]+"/app"))&&C(y,"href",H),16&t&&A(y,"selected","app"===e[4]),e[2]?E?E.p(e,t):(E=ze(e),E.c(),E.m(b,P)):E&&(E.d(1),E=null),e[3]?D?D.p(e,t):(D=Pe(e),D.c(),D.m(b,null)):D&&(D.d(1),D=null)},i(e){L||(ee(h.$$.fragment,e),B((()=>{S||(S=ne(o,$e,{x:-500,duration:600},!0)),S.run(1)})),B((()=>{F||(F=ne(t,xe,{},!0)),F.run(1)})),L=!0)},o(e){te(h.$$.fragment,e),S||(S=ne(o,$e,{x:-500,duration:600},!1)),S.run(0),F||(F=ne(t,xe,{},!1)),F.run(0),L=!1},d(e){e&&g(t),le(h),j&&j.d(),E&&E.d(),D&&D.d(),e&&S&&S.end(),e&&F&&F.end(),_=!1,i(q)}}}function Fe(e,t,o){const n=j();let{id:a}=t,{info:i=""}=t,{title:s}=t,{video:r}=t,{help:l}=t,{tab:h="app"}=t;return e.$$set=e=>{"id"in e&&o(0,a=e.id),"info"in e&&o(7,i=e.info),"title"in e&&o(1,s=e.title),"video"in e&&o(2,r=e.video),"help"in e&&o(3,l=e.help),"tab"in e&&o(4,h=e.tab)},[a,s,r,l,h,n,function(e){"Escape"==e.key&&n("close")},i,()=>n("close")]}class Le extends pe{constructor(e){super(),ce(this,e,Fe,Se,r,{id:0,info:7,title:1,video:2,help:3,tab:4})}}const _e=[{title:"Descriptive statistics and plots",apps:[{id:"asta-b101",title:"Quantiles, quartiles, percentiles",info:"How to compute simple statistics for a sample.",video:"https://www.youtube.com/embed/l_tnFhwRpjI",help:"<h2>Quantiles, quartiles, percentiles</h2><p>This app shows calculation of main non-parametric descriptive statistics: <i>min</i>, <i>max</i>, <i>quartiles</i> and <i>percentiles</i>. The plot contains current sample values as points and the traditional box and whiskers plot. The dashed line inside the box shows the mean. The red elements represent boundaries for detection of outliers (based on ±1.5IQR rule).</p><p>Try to change the smallest (<i>min</i>) or the largest (<i>max</i>) values of your current sample using the sliders in order to see what happens to the boxplot if one of the values will be outside the boundaries. You can also pay attention which statistics are changing and which remain stable in this case.</p><p>The table in the bottom shows the current values (<i>x</i>) ordered from smallest to largest, their rank (<i>i</i>), as well as their percentiles (<i>p</i>) also known as <em>sample quantiles</em>. The percentiles are computed using <code>(i - 0.5)/n</code> rule. The table on the right side shows the computed statistics.</p>"},{id:"asta-b102",title:"Samples and populations",info:"How a sample taken from a population looks like.",video:"https://www.youtube.com/embed/hhGmFVMm5ZE",help:"<h2>Samples and populations</h2> <p>This app helps you to investigate how different a sample can be when it is being randomly taken from corresponding population.</p> <p>You can investigate this difference for one of the three parameters: Height, Age and IQ of a population of people. Each parameter has own distribution. Thus, <em>Age</em> is distributed uniformly, <em>IQ</em> is distributed normally and <em>Height</em> has distribution with two peaks (bimodal). You can also see how sample size influences the difference.</p>  <p>Plot series made for a population (histogram and boxplot on the left part and percentile plot on the right) are shown using gray colors. The size of the population is <em>N</em> = 50&nbsp;000. The plot series for current sample are shown in blue. A new sample is taken when you change any of the controls — select the population parameter or the sample size as well as when you force to take a new sample by clicking the specific button.</p>"},{id:"asta-b103",title:"PDF, CDF and ICDF",info:"Main functions for theoretical distributions.",video:"https://www.youtube.com/embed/lS2iK4Rymy4 ",help:'<h2>PDF, CDF, and ICDF</h2>  <p>This app lets you play with three main functions available for any theoretical distribution: <em>Probability Density Function</em> (PDF), <em>Cumulative Distribution Function</em> (CDF) and <em>Inverse Cumulative Distribution Function</em> (ICDF). The functions can be used for different purposes. Thus PDF shows a shape of distribution in form of a density of the values, the higher density — the bigger chance that your random value will be there. For example, in case of normal distribution, the highest density is around <em>mean</em>, so mean is the most expected value in this case.</p> <p>CDF function gives you a chance to get a value smaller than given. While the ICDF does the opposite — gives you a value for a given probability. The functions in this app can be used in "Value" mode, for a single value, as well as in "Interval" mode for an interval limited by two values.</p> <p>For example, we are talking about height of people, normally distributed with mean = 170 cm and std = 10 cm (initial settings of the app). What is a chance that a random person from this population will have height between 160 and 180 cm? Or, in other words, how many people in percent have height between these two values in the population? Just set <em>x</em><sub>1</sub> to 160 and <em>x</em><sub>2</sub> to 180 under the CDF plot and you will see the result (in this case the chance is around 0.683 or 68.3%).</p>'},{id:"asta-b104",title:"Quantile-quantile plot",info:"How to create and interpret a QQ-plot.",video:"https://www.youtube.com/embed/G12DrRZAPHA",help:"<h2>Quantile-quantile plot</h2><p>This app shows how to use quantile-quantile (QQ) plot to check if your values came from normally distributed population. In this case the values (height of people, <em>x</em>) are indeed randomly taken from a population, where they follow normal distribution with mean = 170 cm and standard deviation = 10 cm. The values of the current sample are shown in the large table as row <em>x</em> and on the plot as y-axis values.</p> <p>First, for every value <em>x</em> we compute probability <em>p</em>, to get a value even smaller, similar to what we did when computed percentiles. In this case we use <code>p = (i - 0.5) / n</code>. But if sample size is smaller than 10, the formula is slightly different: <code>p = (i - 0.375) / (n + 0.25)</code>. For example, if sample size = 6, then the first value (i = 1) will have the following p: <code>p = (1 - 0.375) / (6 + 0.25) = 0.100</code>.</p> <p>After that, for every <em>p</em> we find corresponding standard score, <em>z</em>, using ICDF function for normal distribution. For example, if p = 0.100, the z-score can be found to be equal to -1.28. You can check it using app for PDF/CDF/ICDF or in R by running <code>qnorm(0.100)</code>. Finally we make a plot where sample values, <em>x</em> are shown as y-axis and the <em>z</em>-scores are shown as x-axis. In case if values follow normal distribution ideally they have linear dependence on z-scores, so the points will lie close to a straight line, shown as blue. The closer real points are to this line the more likely that they came from normally distributed population.</p>"}]},{title:"Confidence intervals",apps:[{id:"asta-b201",title:"Population based CI for proportion",info:"Confidence interval for proportion, based on population parameter.",video:"https://www.youtube.com/embed/xdrvR3I10qk",help:"<h2>Population based confidence interval for proportion</h2> <p>This app allows you to play with proportion of a random sample. Here we have a population with N = 1600 individuals. Some of them are red, some are blue. You can change the proportion of the red individuals as you want (by default it is 50%). The population is shown as large plot on the left.</p> <p>If we know proportion of population and sample size we can compute an interval of expected proportions of the future samples. So, when you take a new random sample of that size from the population, its proportion will likely to be inside the interval. This interval is called <em>confidence interval for proportion</em> and since we compute it based on proportion parameter, it is <em>population based</em>.</p> <p>The interval for selected population proportion and current sample size computed for 95% confidence level is shown as a red area under a distribution curve on the right. The vertical line on that plot is a proportion of your current sample. Try to take many samples and see how often the proportion of the sample will be inside the interval (text on the plot shows this information). If you repeat this many (hundreds) times, about 95% of the samples should have proportion within the interval. <strong>However this works only if number of individuals in each group is at least 5.</strong> So if proportion is 10% you need to have sample size n = 50 to meat this requirement.</p>"},{id:"asta-b202",title:"Sample based CI for proportion",info:"Confidence interval for proportion, based on sample statistic.",video:"https://www.youtube.com/embed/3lQRSkjL5ac",help:"<h2>Sample based confidence interval for proportion</h2> <p>This app is similar to <code>asta-b201</code>, but, in this case, confidence interval is computed based on sample proportion. This requires larger sample size, so for every category you need at least 10 individuals in your sample. For example, if proportion is 20%, you need sample size of at least n = 50 to make a reliable interval (20% of 50 is 10). For p = 10% the sample size should be n = 100.</p> <p>The app shows 95% confidence interval computed for current sample as a plot on the right side. So, every time you take a new sample, this also results in a new confidence interval. The vertical red line on this plot shows the population proportion, which in real life we do not know. If you take a new sample many times (say, 200-300) you can see how often the population proportion, π, was inside the interval. If sample size is large enough it should be close to 95% — the confidence level.</p>"},{id:"asta-b203",title:"Population based CI for mean",info:"Confidence interval for mean, based on population parameter.",video:"https://www.youtube.com/embed/cX8ErwtKMc8",help:"<h2>Population based confidence interval for mean</h2> <p>This app is similar to <code>asta-b201</code> but is made to give you an idea about uncertainty of sample mean. Here we have a normally distributed population — concentration of Chloride in different parts of a water source. The concentration has a fixed mean, <em>µ</em> = 100 mg/L, and a standard deviation, <em>σ</em>, which you can vary from 1 to 5 mg/L. The population distribution is shown using gray colors on the left plot. Blue points on that plot show values of a current sample, randomly taken from the population. The vertical lines show the corresponding means.</p> <p>If we know mean of population, <em>µ</em>, and sample size, we can compute an interval of expected mean values of the future samples, <em>m</em>. So, when you take a new random sample of that size from the population, its mean value will likely to be inside the interval. This interval is called <em>confidence interval for mean</em>and since we compute it based on population parameter, it is <em>population based</em>.</p> <p>Right plot shows distribution of possible mean values of samples to be randomly taken from the current population (and for current sample size). Confidence interval, computed for 95% confidence level is shown as a gray area under the distribution curve. The blue vertical line on that plot is a mean of your current sample. Try to take many samples and see how often the mean of a sample will be inside the interval (table under the plot shows this information). If you repeat this many (hundreds) times, about 95% of the samples should have mean within the interval.</p>"},{id:"asta-b204",title:"Sample based CI for mean",info:"Confidence interval for mean, based on sample statistics.",video:"https://www.youtube.com/embed/EgE6-NNyyPc",help:"<h2>Sample based confidence interval for mean</h2> <p>This app is similar to <code>asta-b203</code>, but in this case confidence interval for mean is computed using sample statistics, so we pretend we do not know the population mean and want to estimate it as a value located inside this interval. Thus on the right plot you see distribution and 95% confidence interval computed for current sample. The population mean (which in real life is unknown) is shown as a vertical line.</p> <p>        Try to take many samples and see how often mean of the population will be inside confidence interval computed for the sample. If you repeat this many (hundreds) times, about 95% of the samples will have interval, which contains the population mean. So, before you take a new sample you have 95% chance that confidence interval, computed around the sample mean, will contain the population mean.</p> <p>In this case we use Student's t-distribution to compute the interval. For given confidence level (e.g. 95%) and for given sample size (e.g. 5) we define a critical t-value — how many standard errors the interval will span on each side of the sample mean. E.g. for n = 5 this value is 2.78. You can see this value for current sample size in the table with statistics. If you have R you can also compute this value using ICDF function for t-distribution: <code>qt(0.975, 4)</code>. Here 0.975 is the right boundary of 95% interval and 4 is a number of degrees of freedom, which in this case is equal to <nobr>n - 1</nobr>.</p>"}]},{title:"Hypothesis testing",apps:[{id:"asta-b205",title:"What is p-value?",info:"Explanation of p-value using coin experiment.",video:"https://www.youtube.com/embed/6O7rExp8tCQ",help:"<h2>What is p-value?</h2><p>This app helps to understand the meaning of a p-value in hypotheses testing:</p> <p><em>p-value is a chance to get a sample as extreme as the one you have or even more extreme assuming that the null hypothesis (H0) is true.</em></p> <p>In case if all outcomes of an experiment are equally likely, to compute a p-value we need to know: <em>N1</em> — number of possible outcomes which will be as extreme as the one we currently have, <em>N2</em> — number of outcomes which will be more extreme for given H0, and <em>N</em> — total number of all possible outcomes. In this case the p-value can be computed as: <strong>p = (N1 + N2)/N</strong>.</p> <p>However, when we deal with continuous variables, number of possible outcomes is infinite and different outcomes may have different probabilities, therefore we have to use theoretical distributions for computing chances, which will be also shown in next apps. But in this app we introduce p-values based on experiment with limited number of outcomes — tossing a balanced coin several times (4 or 6). So we can count <em>N1</em>, <em>N2</em> and <em>N</em> and compute the p-value manually.</p>"},{id:"asta-b206",title:"Test for sample proportion",info:"How test for proportion works.",video:"https://www.youtube.com/embed/zU3K4WWx7dI",help:"<h2>Test for sample proportion</h2><p>This app visualizes a test for proportion of a sample — how likely the current sample came from population with given H0. In this case H0 is true, our population indeed has a proportion, π, which we set manually in the app (the population is shown on the left plot). So we expect that the test will confirm the H0 most of the time.</p> <p>Every time you take a new sample, app computes standard error and makes sampling distribution of possible        proportions around π using the computed standard error and normal distribution. After that it evaluates how extreme your sample is and results in a p-value — chance to get a sample with proportion like you have or even more extreme assuming that H0 is true. If you take many samples, e.g. 200 or 300, then only 5% will have a p-value below 0.05, you can see all statistics right on the plot.</p> <p>However, this will work only if sample size is large enough. Try to set the population proportion to π = 0.05 or 0.95. You will see that in this case even sample with n = 40 is too small for the test — sampling distribution curve will be truncated on one side. This leads to two problems — you will see an extreme p-value more often than expected and you have a chance to get a sample with members only from one group, so the sample proportion will be either 0 or 1. In this case standard error is 0 and there is no possibility to make a test. You need much larger sample to make a reliable test for such cases.</p>"},{id:"asta-b207",title:"One sample t-test",info:"Test for mean of one sample.",video:"https://www.youtube.com/embed/PuIns8Y3gjI",help:'<h2>One-sample t-test</h2><p>This app helps to understand how does the one sample t-test work. Here we have a normally distributed population — concentration of Chloride in different parts of a water source. The null hypothesis in this case is made about the population mean, µ, and, depending on a tail, you have the following options — "both": H0: µ = 100 mg/L, "left": µ ≥ 100 mg/L, and "right": µ ≤ 100 mg/L. The population in this app has µ exactly equal to 100 mg/L, so all three hypothesis are true in this case. You have a possibility to change the standard deviation of the population, which by default is set to 3 mg/L but you will see, that it does not influence the outcome of the test.</p> <p>Then you can take a random sample from this population and see how far the mean of the sample is from the mean of the population. The app computes a chance to get a sample as extreme as given or even more extreme assuming that H0 is correct — the <strong>p-value</strong>. Usually p-value is used to assess how extreme your particular sample is for being taken from population where H0 is true. If p-value is small, it is considered as unlikely event and H0 is rejected.</p> <p>Often researchers use 5% (0.05) as a threshold for that. It is called <em>significance limit</em>. You will see that if you take many samples (100 or more), you will find out that approximately 5% of the samples will have p-value below 0.05 although the H0 is true. And this happens regardless the sample size. So this threshold is simply a chance to make a wrong decision by rejection the correct H0. So, if you use 0.05 you have 5% chance to make a wrong decision and e.g. "see" an effect, which does not exist.</p>'},{id:"asta-b208",title:"Power of test and Type II error",info:"How often you will be able to reject wrong H0.",video:"https://www.youtube.com/embed/zUS5HDe5lMk",help:'<h2>Power of test and Type II errors</h2> <p>This app is similar to <code>asta-b207</code> where you played with one-sample t-test. However, in this case you can emulate situations when H0 is not true, meaning the true population mean, µ is different from what you expect by setting H0. The possibilities for H0 are the same, depending on a tail, you have the following options — "left": µ ≥ 100 mg/L, and "right": µ ≤ 100 mg/L. But now you can also change the real population mean and set it to be smaller or larger than 100 mg/L.</p> <p>Try to do this and check how often you will be able to reject H0 (in this case we work with significance level 0.05, so we reject H0 when p-value is below this value). A probability to reject wrong H0 is called a <strong>power of test</strong>. And the situation when you can not reject it is called <strong>Type II</strong> error or false negative. The probability to get Type II error is always opposite to the power of test, e.g. if power is 80% you have 20% chance to make a Type II error.</p> <p>The power of any test depends on several things. First of all it is the test itself — different methods have different power. Second, it depends on the <strong>size of effect</strong> — difference between H0 mean and the real population mean (H1). E.g. if H0 assumes that µ ≤ 100 and the real µ = 105, this difference is 5. Finally, power also depends on standard deviation of your population as well as on the sample size. The last has very important consequence — the smaller effect you want to detect, the larger sample size should be.</p>'}]},{title:"Comparing means",apps:[{id:"asta-b209",title:"Two sample t-test",info:"How to compare mean of two samples.",video:"https://www.youtube.com/embed/OEA5l04eVdU",help:"<h2>Two sample t-test</h2><p>This app shows how to compare means of two samples. In this case the objective is to find out if the samples were taken from populations with the same means (H0: µ1 = µ2) or not (H1: µ1 ≠ µ2). Here we use this test to see if increasing a temperature influence the yield of a chemical reaction. So, the population 1 consists of all possible outcomes of the reaction running at T = 120ºC. The population 2 consists of all possible outcomes of the reaction running at T = 160ºC. We assume that there are no other systematic factors involved so the variation of yield within each population is totally random and is distributed normally. The left plot shows the corresponding distributions using blue and red colors.</p> <p>By default µ1 = µ2 = 100 mg. Since µ1 – µ2 = 0, we can say that in this case <em>temperature does not have any effect on yield</em>. However, if we run the reactions just a few times (e.g. 3 for each temperature) you will always observe an effect and therefore you need to asses how likely you observe it just by chance.</p> <p>Use the app and investigate how often you will see an effect, which is not present and, vice versa, how often you will not be able to detect an existent effect. Check how the real (expected) effect size, noise and sample size influence this ability. The app works using significance level 0.05 but remember that for real applications it is better to use smaller value for the level.</p>"},{id:"asta-b210",title:"Multiple comparison and Bonferroni correction",info:"What if we apply t-test to more than 2 groups.",video:"https://www.youtube.com/embed/1qh7Ibfeveg",help:"<h2>Multiple t-test and Bonferroni correction</h2> <p>This app shows how to compare three samples taken from three populations. The three populations are all outcomes (yield measured in mg/L) of a chemical process running with a catalyst A, catalyst B and catalyst C. Here H0: µA = µB = µC = 100 mg/L. This means that regardless which catalyst we use, the average yield of the reaction is 100 mg/L, so changing catalyst has no effect on the yield. But when we run the reaction only 5 times for each catalyst, like shown in the app, the mean of these 5 runs will not be the same as the expected mean of the populations. And most of the time you will observe a difference among the sample means. Our goal is to use a t-test to test the H0 and make decision.</p> <p>However, t-test can be applied for comparing mean of two samples, while here we have three. One of the possibility will be to run t-test three times — one for each pair. This is what is called a <em>multiple compare</em> — you compare samples using several tests to check a single hypothesis. But the more tests you do the higher chance that you will reject correct H0. Try to run the test many times and you will see that although app works at significance limit 0.05 (so we expect that the H0 will be incorrectly rejected in 5% of cases), the real percent of rejections will be higher, about 10%.</p><p>You can overcome this problem by using Bonferroni correction, which decreases the significance limit in each individual tests, so the overall significance will be 0.05 (or any other pre-defined value). You can see the effect of correction by turning it on in the app and repeating the sampling many times again. In this case the significance level for individual tests will be set to 0.05/3 ≈ 0.017 and the number of incorrectly rejected H0 will be around 5%.</p>"},{id:"asta-b211",title:"One-way ANOVA (simplified)",info:"How Analysis of Variance works for one factor.",video:"https://www.youtube.com/embed/NMaIEHWkI5A",help:'<h2>One way ANOVA</h2> <p>This app shows how one-way ANOVA tests means of three samples — the outcomes of a chemical reaction running using three different catalysis: <em>A</em>, <em>B</em> and <em>C</em>. We "run" the reaction with each catalyst 5 times, which gives 15 values — yield of each run in mg. The obtained yield values are shown in the top left table. The last row shows the average yield for each catalyst. You can adjust the expected effect for each catalyst and noise using slider controls.</p> <p>Then app computes a global mean for all original values and subtract it from the values thus creating a table with unbiased values, which are shown in the gray column. Table in the top of the column contains the unbiased values and their means. Under the table there are statistics: degrees of freedom (DoF), sum of squared values (SSQ) and variance or mean squares (MS = SSQ/DoF). Plot below shows boxplots for populations and points for the values.</p> <p>After that we split the unbiased values into a sum of <em>systematic</em> part, shown in the green column, and the <em>residuals</em>, shown in the red column. In the systematic part we assume there is no noise, so all outcomes for given factor level (e.g. column A) have the same value — the corresponding mean. Residuals are computed as a difference between the unbiased values and the systematic part. App computes DoF, SSQ and MS for each part and the F-value — which is a ratio of MS for systematic part and residuals. The F-value follows F-distribution shown under the original data table. We use this distribution to compute corresponding p-value and make decision about the H0.</p>'},{id:"asta-b212",title:"One-way ANOVA (full)",info:"A more detailed app.",video:"https://www.youtube.com/embed/k738X17uNUc",help:"<h2>One way ANOVA (full)</h2> <p>This app is almost identical to the <code>asta-b211</code> but here we show calculations as they are without subtracting the global mean in advance. The results are absolutely identical but this time without additional step of unbiasing the values. Plus the app shows importance of QQ plot for residuals which helps to assess their normality.</p>"}]},{title:"Covariance and regression",apps:[{id:"asta-b301",title:"Covariance",info:"How to compute and understand the covariance.",video:"https://www.youtube.com/embed/-zDtbk33ZOI",help:"<h2>Covariance</h2> <p>This app helps to understand covariance — a statistic which tells if two variables, <em>x</em> and <em>y</em> have a linear relationship (co-vary). If covariance is positive, then increasing <em>x</em> will likely lead to increasing of <em>y</em> value and vice versa. To compute the covariance, we first calculate distance from x- and y-value of a data point to corresponding means and then take a product of the two distances. The covariance is a sum of the distance products divided to the number of degrees of freedom (n - 1). You can see all these calculations in a table.</p> <p> Try to change parameters of a population: amount of noise and a slope of best fit line which has mean values as the origin. You will see how this influences your sample, and the sample co-variance. If product of two distances is positive this point contributes positively to the covariance and such point and the corresponding row in the table is shown using red color. If product of the two distances is negative — blue color is used.</p>"},{id:"asta-b302",title:"Correlation and population based CI",info:"Pearson's correlation coefficient and population based CI.",video:"https://www.youtube.com/embed/zDP6sr4YAC8",help:"<h2>Correlation and population based confidence interval</h2> <p>This app helps you to understand the Pearson's correlation coefficient, <em>r(x,y)</em>, which is computed as covariance for standardized <em>x</em> and <em>y</em> values. Alternatively you can compute covariance for the original values and then standardize the covariance by dividing it to the standard deviation of  <em>x</em> and <em>y</em>. If there is no noise at all, and <em>y</em> is linearly dependent on <em>x</em>, the correlation does not depend on slope of the line. However, when noise is present, the slope has an influence which you can see by playing with the app. The right column in the table with statistics (shown as gray) shows values for population, the middle column shows values for a current sample.</p> <p>The uncertainty for correlation coefficient of a sample depends both on the correlation of population and the sample size. The sample correlation coefficient does not follow any theoretical distribution, therefore for computing the uncertainty and corresponding confidence interval, a <a href=\"https://en.wikipedia.org/wiki/Fisher_transformation\">transformed statistic</a>, <em>z'</em>, is used. This statistic follows normal distribution if n > 10. The app shows how the distribution of <em>z'</em> looks like for different levels of noise and how it can be transformed back to distribution of <em>r</em> values.</p>"},{id:"asta-b303",title:"Correlation and sample based CI",info:"Pearson's correlation coefficient and sample based CI.",video:"https://www.youtube.com/embed/cq2IytKelB4",help:"<h2>Correlation and sample based confidence interval</h2> <p>This app is almost identical to the previous one (asta-b302) with one important difference: confidence interval in this app is computed based on statistics of a current sample. So, you can see how confidence interval vary from one sample to another and how often the correlation coefficient of population (or it's trasformed value, z') will be inside the interval.</p> <p>Because the confidence intervals in this app are computed for 95% confidence level, you can expect that in 95% of all cases sample will contain the population parameter inside the interval. However, you will see exactly 95%, only if you take a large amount of samples, several hundreds or even thousands. This is similar to confidence intervals computed for other statistics, e.g. mean or proportion.</p>"},{id:"asta-b304",title:"Simple linear regression",info:"SLR and its main outcomes.",video:"https://www.youtube.com/embed/j8po0gDiLrY",help:"<h2>Simple linear regression</h2> <p>The app shows how to use simple linear regression for investigation of relationship between two variables (in this case height and weight of adult persons). The plot shows data points both for population (N = 500) and current sample (n = 10). Both sets of points are fitted by a simple linear regression model, you can see both models in form of lines and the corresponding equations, as well as their characteristics (standard error of  prediction and coefficient of determination, R2). The table on the right part of the app shows reference y-values, values, predicted by the model, error of prediction and its square. Sum of squared errors is what is used to compute both standard error and R2.</p> <p>The shaded area on the plot shows uncertainties. By default you see uncertainty from both fitting and sampling error. You can use the switch to see uncertainty from one of the source. You can also change the amount of noise (the more noise, the less percent of y-variance can be predicted by the model) and see how it changes the uncertainties. Plus you can select any sample point on the plot and see the predicted value and the uncertainty interval for this point.</p>"},{id:"asta-b305",title:"Sampling error and overfitting",info:"How sampling error depends on sample size and model complexity.",video:"https://www.youtube.com/embed/ARAB_8Q5okM",help:'<h2>Sampling error and overfitting</h2> <p>This app shows how sample size and complexity of a regression model influence the sampling error. Sampling error in this case can be defined as variation of regression coefficients of a model, trained on a sample, around the "true" regression coefficients of a model trained on the population points. This app uses polynomial model for regression — the higher polynomial degree the higher the model complexity.</p> <p>Just set the desired sample size and the polynomial degree and then start collecting new samples. Points and model for the current sample are shown using red color for better contrast. The models for all previous samples are kept on the main plot (they are also red but semi transparent), so you can see how big the variation of the models is.</p> <p>The small plot on the right shows regression coefficients. The semi-transparent blue bars show the "true" regression coefficients for the population. Red points are regression coefficients of current and all previous samples. So you can see how big the variation of the coefficients is and how it depends on sample size and model complexity.</p>'},{id:"asta-b306",title:"Cross-validation",info:"Cross-validation, model performance and overfitting.",video:"https://www.youtube.com/embed/9E8XRowOsPY",help:"<h2>Cross-validation</h2> <p>Cross-validation is a way to estimate the sampling error without taking new samples from the population. The idea is to split your original observations to several segments and then, for each segment, make a local model by taking the segment observations out of the dataset, and use the rest for training the model. After that, the local model is used to predict the response values of the excluded observations. The procedure is repeated for each segment, so at the end every observation will have a predicted response value (<em>y</em><sub>cv</sub>).</p> <p>Splitting the observations into segments can be done in several ways. The simplest is to consider every observation as individual segment. In this case the number of segments is equal to the number of observations and thus on every step one observation will be taking out, while the rest will be used for training the local model. This way is called <em>leave-one-out</em> or <em>full cross-validation</em>. Alternatively you can assign two or more observations to each segment. This can be done randomly (<em>random segmented cross-validation</em>) or systematically by taking every k-th observation (<em>venetian blinds</em>).</p> <p>This app shows how all three methods work for a simple dataset with 12 observations. If random or systematic split is selected, number of segments will be equal to 4. Cross-validation can help to detect overfitting, you can also test this in the app — try to use overfitted model, e.g. cubic polynomial and will see that despite the calibration error is getting smaller, the cross-validation error will be larger for overcomplicated models.</p>"},{id:"asta-b307",title:"Jackknifing",info:"Jackknife resampling for regression coefficients.",video:"https://www.youtube.com/embed/3tA85B4V2kI",help:"<h2>Jackknife resampling for regression coefficients</h2><p>Jackknife resampling is a way to make an inference (e.g. compute confidence intervals of find a p-value for H0: β = 0) for regression coefficients based on full cross-validation. The idea is to estimate a variance of regression coefficients computed for each local model (every time we take one sample out, we compute a new model, which is called <em>local</em>) and then compute the standard error for the coefficient based on this variance.</p> <p> In this app population consists of 500 measurements of <em>x</em> and <em>y</em>, where <em>y</em> linearly depends on <em>x</em>. However, if you take a small sample (in this app the sample size is fixed to <em>n</em> = 12), it can have a random non-linear effect in the <em>y</em>(<em>x</em>) relationship, so when this sample is fitted by a polynomial model, the regression coefficients for quadratic or cubic terms will not be zero. But if you use Jackknife, then most of the time it will be able to detect that the estimated coefficients are in fact not significant, so statistically they are not distinguishable from zero.</p> <p> In order to investigate this, set polynomial to <em>cubic</em> and take a sample where the last two regression coefficients will be relatively large, so you can see them as small blue bars on the coefficients' plot. Then run the cross-validation procedure and see how all coefficients vary for the local models. At the end you will see errorbars which correspond to 95% confidence intervals and for the non-linear terms, most of the time, the interval will cross zero.</p>"},{id:"asta-b308",title:"Multiple Linear Regression",info:"Multiple Linear Regression.",video:"https://www.youtube.com/embed/pVryGi1_K2c",help:'<h2>Multiple linear regression model</h2> <p>This app helps to understand a Multiple Linear Regression model, where a response variable (<em>y</em>) depends on two predictors (<em>X</em><sub>1</sub> and <em>X</em><sub>2</sub>) as well as on their interaction. The model is represented by a set of four coefficients, <em>b</em><sub>0</sub> (bias or intercept), <em>b</em><sub>1</sub> (effect of <em>X</em><sub>1</sub>), <em>b</em><sub>2</sub> (effect of <em>X</em><sub>2</sub>) and <em>b</em><sub>12</sub> (effect of interaction between <em>X</em><sub>1</sub> and <em>X</em><sub>2</sub>). You can change the values of the coefficients using corresponding controls.</p><p>The model is visualized as a surface in (<em>X</em><sub>1</sub>, <em>X</em><sub>2</sub>, <em>y</em>) Cartesian space as it is shown on the 3D plot. The model surface is represented by a two sets of parallel straight lines. One set of lines shows how <em>y</em> depends on <em>X</em><sub>1</sub>, when <em>X</em><sub>2</sub> has different fixed values (one line for each fixed <em>X</em><sub>2</sub>). The second set of lines, which is orthogonal to the first set, shows how <em>y</em> depends on <em>X</em><sub>2</sub> when <em>X</em><sub>1</sub> has different fixed values. You can show both sets or only one of them by using control element "Show lines".</p> <p>The 3D scene can be rotated and zoomed in/out. You can do it with a mouse (drag for rotation and scroll for zooming) or by keyboard (arrows for rotation and "+", "-" for zooming). You can also see a selected point whose X-coordinates you can change. As well as the equation, which shows how y-value of this point is computed using the current model.</p>'},{id:"asta-b309",title:"MLR and colinearity",info:"MLR and colinearity.",video:"https://www.youtube.com/embed/iRY8lklHMjc",help:"<h2>Colinearity in MLR</h2><p>This app demonstrates how co-linearity can affect an MLR model. <em>Colinearity</em> is a situation when two or several predictors (<em>x</em>-variables) are correlated (have linear relationship). This can cause some problems when fitting an MLR model, as it implies the lack of correlation among the predictors. If correlation is above moderate, this leads to larger uncertainty between the estimated and expected regression coefficients. But If correlation is high/strong, this can lead to a very large uncertainty and makes the model uninterpretable. In some cases it can even make the fitting impossible.</p><p>The severity of the problem depends on several things. First of all, on how strong the correlation among the predictors is. Usually MLR is stable to weak or moderate correlations. Second factor is the sample size, the smaller number of observations the bigger the uncertainty is. And, finally, it also depends on the fitting error — how well y-values are fitted by the model. In this app you can change all these parameters and then take several samples and see how big will be the uncertainty and how far is the MLR plane from the expected. Start with default settings which gives the best model, take several samples and check how far the regression coefficients of the fitted model are from the expected/theoretical shown with gray bars. Then play with the sample quality parameters and check how do they influence the fitting quality.</p>"}]},{title:"Principal component analysis",apps:[{id:"mda-b201",title:"PCA: distances and variances",info:"How PCA fits the data.",video:"",help:"<h2>PCA: distances and variances</h2> <p>This app shows how data can be represented using Principal Component. The data consists of measurements done for 10 objects and 2 variables, <i>x</i><sub>1</sub> and <i>x</i><sub>2</sub>, which are correlated (you can think of, for instance, height and weight of 10 persons or absorbance at two closely located wavelengths for 10 mixtures). Hence the original dataset is two dimensional and consists of 20 numbers in total. The data points are shown on the plot with blue color. The coordinates of the points are shown in the table.</p> <p>Because of the correlation, we can represent the data using one latent variable instead of the original two. This latent variable (LV) is shown as an orange line and you can change its orientation. If you project the data onto this latent variable, then the data points will be represented by only one value each — position of the projected point along the LV. This position is called a <em>score</em> and is denoted by a letter <em>t</em> in the table. The projected points are shown with orange color on the plot. You can manually change the orientation of the LV and see how well it describes the variation of the data. The best orientation in this case will be the one, which gives spread of the orange points as close as possible to the spread of the blue points. There is also a button <em>Fit</em> which find the best orientation of the LV that maximizes this spread. In this case the LV is called a <em>Principal Component</em>.</p> <p>Usually, spread of points is measured as a sum of squared distances from the points to the origin. You can see two such sums on the plot: the sum of squared distances for the original (blue) points and the sum of squared distances computed for projected (orange) points. There is also a third sum — for squared distances between the two points. If you click on any individual point you will see these three distances for this point. The numerical values of the individual squared distances are also shown in the table.</p>"},{id:"mda-b202",title:"PCA NIPALS algorithm",info:"How to find orientation of PCs.",video:"",help:"<h2>PCA NIPALS algorithm</h2> <p>This app shows how to compute coordinates of loading vector — a unit vector which defines a direction of Principal Component — using Nonlinear Iterative Partial Least Squares (NIPALS) algorithm. This is a simplified example where data is two-dimensional, represented by two variables, <em>x</em><sub>1</sub> and <em>x</em><sub>2</sub> and we fit this data with one single Principal Component, PC1, similar to the previous app (mda-b201).</p> <p>NIPALS finds the optimal orientation of PC (which maximizes explained variance) as follows. First of all, the variables are mean centred, so mean value for each variable is zero. Then an initial selection for PC1 is chosen, usually we simply take a direction of one of the variables. This case app choses  <em>x</em><sub>2</sub> as initial orientation of PC1. This initial step is a step 0. After that we project all points to PC1 and get scores, <em>t</em>.</p> <p>To get new coordinates of the loadings (for every next step), we do the following. First we compute covariance between <em>x</em><sub>1</sub> and the current scores values, cov(<em>x</em><sub>1</sub>, <em>t</em>), and take the computed covariance  value as <em>x</em><sub>1</sub> coordinate of the future loading vector. Then we compute covariance between <em>x</em><sub>2</sub> and the current scores values, cov(<em>x</em><sub>2</sub>, <em>t</em>), and take the computed value as <em>x</em><sub>2</sub> coordinate of the future loading vector. Finally we normalize the coordinates to make sure that the length of the new vector is one. You can see all these calculations in the right bottom part of the plot.</p> <p>And then we simply repeat this step over and over until the difference between coordinates of the loading-vector is smaller than a pre-defined threshold. Alternatively we can set a threshold to the difference in explained variance. Try to play with the steps and you will see that the first step always gives the biggest difference in orientation of the PC. The second step makes a small correction. Correction from the step 3 and further steps are barely visible by naked eye.</p>"},{id:"mda-b203",title:"Elements of PCA model",info:"Visual representation of main PCA outcomes.",video:"",help:"<h2>Elements of PCA model</h2> <p>This app helps to understand the meaning of all individual elements of the PCA model: <strong>X = TP<sup>T</sup> + E</strong>, providing visual representation of all values from this equation. In this case we have a dataset which consists of 5 points with fixed coordinates. The latent variable can be rotated or your can fit it according to PCA requirement, so it will be oriented to capture the biggest variance of the data points.</p> <p>If you select any point on the plot you will see details for this particular point. There will be two show modes: 'score' and 'all' which you can switch between (the switcher is disabled if no points are selected). In mode 'score' you will see a simplified representation — original point (blue), its projection (orange), the corresponding score (orange number) as well as direction and coordinates of the loading vector, which defines orientation of the latent variable (red). You will also see table with calculation details for scores: <strong>T = XP</strong>.</p> <p>If you select show mode 'all' you will see representation of the projected point in original variable space — the <strong>TP<sup>T</sup></strong> component of the PCA model. The difference between these coordinates and the coordinates of the original points forms the matrix <strong>E</strong>.</p>"}]}];function qe(e,t,o){const n=e.slice();return n[15]=t[o],n}function je(e){let t,n;const a=[e[4],{tab:e[3]}];let i={};for(let e=0;e<a.length;e+=1)i=o(i,a[e]);return t=new Le({props:i}),t.$on("close",e[9]),{c(){se(t.$$.fragment)},m(e,o){re(t,e,o),n=!0},p(e,o){const n=24&o?ae(a,[16&o&&ie(e[4]),8&o&&{tab:e[3]}]):{};t.$set(n)},i(e){n||(ee(t.$$.fragment,e),n=!0)},o(e){te(t.$$.fragment,e),n=!1},d(e){le(t,e)}}}function Ee(e){let t,n;const a=[e[15]];let i={};for(let e=0;e<a.length;e+=1)i=o(i,a[e]);return t=new Te({props:i}),{c(){se(t.$$.fragment)},m(e,o){re(t,e,o),n=!0},p(e,o){const n=2&o?ae(a,[ie(e[15])]):{};t.$set(n)},i(e){n||(ee(t.$$.fragment,e),n=!0)},o(e){te(t.$$.fragment,e),n=!1},d(e){le(t,e)}}}function De(e){let t,o,n,a,s,r,l,h,c,p,d,u,m,w=e[2]&&e[4]&&je(e),b=e[1].filter(Ne),z=[];for(let t=0;t<b.length;t+=1)z[t]=Ee(qe(e,b,t));const P=e=>te(z[e],1,1,(()=>{z[e]=null}));return{c(){w&&w.c(),t=x(),o=T("div"),n=T("input"),a=x(),s=T("button"),s.textContent="×",r=x(),l=T("span"),h=k(e[5]),c=x();for(let e=0;e<z.length;e+=1)z[e].c();p=k(""),C(n,"placeholder","Enter a single keyword (e.g. interval)"),C(n,"class","svelte-12lrilt"),C(s,"class","svelte-12lrilt"),A(s,"hidden",e[0].length<1),C(l,"class","svelte-12lrilt"),C(o,"class","search-block svelte-12lrilt")},m(i,b){w&&w.m(i,b),v(i,t,b),v(i,o,b),f(o,n),H(n,e[0]),f(o,a),f(o,s),f(o,r),f(o,l),f(l,h),v(i,c,b);for(let e=0;e<z.length;e+=1)z[e].m(i,b);v(i,p,b),d=!0,u||(m=[$(window,"load",e[7]),$(window,"hashchange",e[7]),$(n,"keydown",e[6]),$(n,"input",e[10]),$(s,"click",e[11])],u=!0)},p(e,[o]){if(e[2]&&e[4]?w?(w.p(e,o),20&o&&ee(w,1)):(w=je(e),w.c(),ee(w,1),w.m(t.parentNode,t)):w&&(G(),te(w,1,1,(()=>{w=null})),Z()),1&o&&n.value!==e[0]&&H(n,e[0]),1&o&&A(s,"hidden",e[0].length<1),(!d||32&o)&&I(h,e[5]),2&o){let t;for(b=e[1].filter(Ne),t=0;t<b.length;t+=1){const n=qe(e,b,t);z[t]?(z[t].p(n,o),ee(z[t],1)):(z[t]=Ee(n),z[t].c(),ee(z[t],1),z[t].m(p.parentNode,p))}for(G(),t=b.length;t<z.length;t+=1)P(t);Z()}},i(e){if(!d){ee(w);for(let e=0;e<b.length;e+=1)ee(z[e]);d=!0}},o(e){te(w),z=z.filter(Boolean);for(let e=0;e<z.length;e+=1)te(z[e]);d=!1},d(e){w&&w.d(e),e&&g(t),e&&g(o),e&&g(c),y(z,e),e&&g(p),u=!1,i(m)}}}const Ne=e=>e.apps.length>0;function Re(e,t,o){let n,a,i,s,r=!1,l="",h="";function c(e,t){o(4,s=function(e){const t=_e.map((t=>({apps:t.apps.filter((t=>"#"+t.id===e))}))).filter((e=>e.apps.length>0));return 0===t.length||0===t[0].apps.length?null:t[0].apps[0]}(e)),s?(["app","video","info"].includes(t)||(t="app"),"video"==t&&""===s.video&&(t="app"),o(2,r=!0),o(3,l=t||"app"),document.querySelector("body").style.overflow="hidden"):location.hash=""}function p(e){document.querySelector("body").style.overflow="auto",o(2,r=!1),location.hash="",o(3,l=""),o(4,s=void 0)}function d(e){void 0!==e&&"Escape"!==e.key||o(0,h="")}return e.$$.update=()=>{1&e.$$.dirty&&o(1,n=h.length>1?_e.map((e=>({title:e.title,apps:e.apps.filter((e=>e.title.toLowerCase().search(h.toLowerCase())>=0|e.info.toLowerCase().search(h.toLowerCase())>=0))}))):_e),2&e.$$.dirty&&o(8,a=n.reduce(((e,t)=>parseInt(e)+t.apps.length),0)),257&e.$$.dirty&&o(5,i=h.length>0?`Found ${a} app${a>1?"s":""}`:`${a} apps in the list.`)},[h,n,r,l,s,i,d,function(){if(""===location.hash)return void(r&&p());const e=location.hash.split("/"),t=e[0],o=e[1];""!==t?c(t,o):r&&p()},a,()=>location.hash="",function(){h=this.value,o(0,h)},()=>d(void 0)]}return new class extends pe{constructor(e){super(),ce(this,e,Re,De,r,{})}}({target:document.getElementById("app-list")})}();
//# sourceMappingURL=bundle.js.map
